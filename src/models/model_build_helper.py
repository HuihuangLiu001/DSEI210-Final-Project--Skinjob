# -*- coding: utf-8 -*-
"""model_build_helper.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sy9R1nDE4bYH0ytNurWjw5m7trHbYz76
"""

import numpy as np 
import pandas as pd
import tensorflow as tf

from PIL import Image

from tensorflow import keras
import tensorflow_docs as tfdocs
import tensorflow_docs.modeling
import tensorflow_docs.plots
from tensorflow.keras import regularizers
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from keras.utils.np_utils import to_categorical
import keras_tuner as kt
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
from matplotlib.pyplot import imshow

# model builder with tuning
def model_builder_tuning(hp):
  '''Take a set a of variables
    Return a CNN model with EfficientNetB1 ready for training only for 90x90 iamge
  '''
  inputA = tf.keras.Input(shape=(90, 90, 3))
  inputB = tf.keras.Input(shape=(20,))
  # Create the base model with EfficientNetB1
  base_model = tf.keras.applications.EfficientNetB1(include_top=False, weights='imagenet', pooling='max')(inputA)
  Normalizator_layer = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(base_model)
  # Choose an optimal value between 32-512
  hp_units = hp.Int('units', min_value=128, max_value=512, step=32)
  x = tf.keras.layers.Dense(hp_units, activation="relu")(Normalizator_layer)
  image_part = tf.keras.Model(inputs=inputA, outputs=x)

  y = tf.keras.layers.Dense(8, activation="relu")(inputB)
  cate_part = tf.keras.Model(inputs=inputB, outputs=y)

  combined = tf.keras.layers.concatenate([image_part.output, cate_part.output])

  z = tf.keras.layers.Dense(128,
                            kernel_regularizer = regularizers.l2(l = 0.02),
                            activation='relu')(combined)
  z = keras.layers.Dropout(rate=.5, seed=42)(z)
  prediction_layer = keras.layers.Dense(7, activation='softmax')(z)

  model = tf.keras.Model(inputs=[image_part.input, cate_part.input], outputs=prediction_layer)

  # Choose an optimal value from 0.01, 0.001, or 0.0001
  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])
  model.compile(optimizer=tf.keras.optimizers.Adamax(learning_rate=hp_learning_rate),
              loss='categorical_crossentropy',
              metrics=['accuracy'])
  return model

# model builder with meta data include for CNN
def model_builder_c(img_shape, nodes, lr):
  '''Take a set for image shape, a int of nodes and a float for learning rate
    Return a CNN model with EfficientNetB1 ready for training
  '''
  inputA = tf.keras.Input(shape=img_shape)
  inputB = tf.keras.Input(shape=(20,))
  # Create the base model with EfficientNetB1
  base_model = tf.keras.applications.EfficientNetB1(include_top=False, weights='imagenet', pooling='max')(inputA)
  Normalizator_layer = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(base_model)
  x = tf.keras.layers.Dense(nodes, activation="relu")(Normalizator_layer)
  image_part = tf.keras.Model(inputs=inputA, outputs=x)

  y = tf.keras.layers.Dense(8, activation="relu")(inputB)
  cate_part = tf.keras.Model(inputs=inputB, outputs=y)

  combined = tf.keras.layers.concatenate([image_part.output, cate_part.output])

  z = tf.keras.layers.Dense(128,
                            kernel_regularizer = regularizers.l2(l = 0.02),
                            activation='relu')(combined)
  z = keras.layers.Dropout(rate=.5, seed=42)(z)
  prediction_layer = keras.layers.Dense(7, activation='softmax')(z)

  model = tf.keras.Model(inputs=[image_part.input, cate_part.input], outputs=prediction_layer)

  model.compile(optimizer=tf.keras.optimizers.Adamax(learning_rate=lr),
              loss='categorical_crossentropy',
              metrics=['accuracy'])
  return model

# model builder without meta data include for CNN
def model_builder(img_shape, nodes, lr):
  '''Take a set for image shape, a int of nodes and a float for learning rate
    Return a CNN model with EfficientNetB1 ready for training
  '''
  # Create the base model with EfficientNetB1
  base_model = tf.keras.applications.EfficientNetB1(input_shape=img_shape, include_top=False, weights='imagenet', pooling='max')

  Normalizator_layer = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )

  prediction_layer = keras.layers.Dense(7, activation='softmax')

  model = tf.keras.Sequential([
    base_model,
    Normalizator_layer,
    keras.layers.Dense(nodes, 
                      kernel_regularizer = regularizers.l2(l = 0.02),
                      activation='relu'),
    keras.layers.Dropout(rate=.5, seed=42),
    prediction_layer                           
  ])
  model.compile(optimizer=tf.keras.optimizers.Adamax(learning_rate=lr),
                loss='categorical_crossentropy',
                metrics=['accuracy'])
  return model