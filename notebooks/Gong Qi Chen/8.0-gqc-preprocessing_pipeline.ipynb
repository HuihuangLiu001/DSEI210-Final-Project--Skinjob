{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing_pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yhXMo64KnYDz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e85eface-f5fa-4bef-f25f-bd318e28d28d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content\""
      ],
      "metadata": {
        "id": "jVF3KVbpnhvY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d surajghuwalewala/ham1000-segmentation-and-classification"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytdj2gMDnh3X",
        "outputId": "826ac511-395d-42ef-f1f9-c2ef0b379be8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /content/kaggle.json'\n",
            "Downloading ham1000-segmentation-and-classification.zip to /content\n",
            "100% 2.59G/2.59G [00:34<00:00, 105MB/s] \n",
            "100% 2.59G/2.59G [00:34<00:00, 80.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/ham1000-segmentation-and-classification.zip"
      ],
      "metadata": {
        "id": "_ZS7qftnnoBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/tensorflow/docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5GP0nfznqHn",
        "outputId": "b37eba10-07ea-4811-d2fa-098a98239524"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/tensorflow/docs\n",
            "  Cloning https://github.com/tensorflow/docs to /tmp/pip-req-build-l1wocnxl\n",
            "  Running command git clone -q https://github.com/tensorflow/docs /tmp/pip-req-build-l1wocnxl\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from tensorflow-docs==0.0.0.dev0) (0.8.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow-docs==0.0.0.dev0) (1.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-docs==0.0.0.dev0) (2.11.3)\n",
            "Requirement already satisfied: protobuf>=3.14 in /usr/local/lib/python3.7/dist-packages (from tensorflow-docs==0.0.0.dev0) (3.17.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from tensorflow-docs==0.0.0.dev0) (3.13)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.14->tensorflow-docs==0.0.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->tensorflow-docs==0.0.0.dev0) (2.0.1)\n",
            "Building wheels for collected packages: tensorflow-docs\n",
            "  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorflow-docs: filename=tensorflow_docs-0.0.0.dev0-py3-none-any.whl size=179891 sha256=0c85488ee9314161192a6b591c23723ed5a72a3be1382d0aabc271afe7f56f6b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-em29cexy/wheels/cc/c4/d8/5341e93b6376c5c929c49469fce21155eb69cef1a4da4ce32c\n",
            "Successfully built tensorflow-docs\n",
            "Installing collected packages: tensorflow-docs\n",
            "Successfully installed tensorflow-docs-0.0.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from tensorflow import keras\n",
        "import tensorflow_docs as tfdocs\n",
        "import tensorflow_docs.modeling\n",
        "import tensorflow_docs.plots\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "from matplotlib.pyplot import imshow"
      ],
      "metadata": {
        "id": "c0ku0eDGnr8e"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def undersample(df, count, rs=42):\n",
        "  result=[]\n",
        "  df_group = df.groupby('dx')\n",
        "  for x in df['dx'].unique():\n",
        "    group = df_group.get_group(x)\n",
        "    num = int(group['dx'].value_counts())\n",
        "    if num >= count:\n",
        "      s=group.sample(count, axis=0, random_state=rs)\n",
        "    else:\n",
        "      s=group.sample(frac=1, axis=0, random_state=rs)\n",
        "    result.append(s)\n",
        "  return pd.concat(result, axis=0).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "Z-W7Z-01nsfO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def oversample(df, count, rs=42):\n",
        "  lst = [df]\n",
        "  for class_index, group in df.groupby('dx'):\n",
        "      lst.append(group.sample(count-len(group), replace=True, random_state=rs))\n",
        "  df_new = pd.concat(lst)\n",
        "  return df_new"
      ],
      "metadata": {
        "id": "bf_q0D-Rn6pI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def img_np_convert(df, image_path, h, w):\n",
        "  df['image_id'] = image_path + df['image_id'] +'.jpg'\n",
        "  df['image'] = df['image_id'].map(lambda x: np.asarray(Image.open(x).resize((h, w))).astype(np.float32))\n",
        "  return df"
      ],
      "metadata": {
        "id": "uc7AUUKwn8-M"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def img_np_convert_scaled(df, image_path, h, w):\n",
        "  df['image_id'] = image_path + df['image_id'] +'.jpg'\n",
        "  df['image'] = df['image_id'].map(lambda x: (np.asarray(Image.open(x).resize((h, w)))/255).astype(np.float32))\n",
        "  return df"
      ],
      "metadata": {
        "id": "GhPwzJesaKmD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def my_split(df, train_size, test_size, val_size, rs=42):\n",
        "  valid_test_split = val_size / test_size\n",
        "\n",
        "  df_train, df_test_val = train_test_split(df, test_size=1-train_size, shuffle=True, random_state=rs)\n",
        "  df_val, df_test = train_test_split(df_test_val, test_size=valid_test_split, shuffle=True, random_state=rs)\n",
        "\n",
        "  df_train.reset_index(inplace=True)\n",
        "  return df_train, df_test, df_val"
      ],
      "metadata": {
        "id": "pLw6PG3ToHUu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weight_cal(df):\n",
        "  class_weight={}\n",
        "  labels = list(df['dx'].unique())\n",
        "  labels.sort()\n",
        "  count = df['dx'].value_counts()\n",
        "  for idx in range(7):\n",
        "    class_weight[idx] = count['nv']/count[labels[idx]]\n",
        "  return class_weight, labels"
      ],
      "metadata": {
        "id": "fIIBDTFJpV3u"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def df_to_np1(df):\n",
        "  image = np.asarray(df['image'].to_list()) \n",
        "  target_df = df['dx']\n",
        "  target = pd.get_dummies(data=target_df, columns=['dx']).to_numpy()\n",
        "  return image, target"
      ],
      "metadata": {
        "id": "beB9J2U_qDCk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def df_to_np2(df):\n",
        "  df_feature = df[['image', 'age', 'dx_type', 'localization']]\n",
        "  df_feature = pd.get_dummies(data=df_feature, columns=['dx_type', 'localization'])\n",
        "  df_feature['image'] = df_feature['image'].map(lambda x : x.flatten())\n",
        "  i_feature = np.asarray(df_feature['image'].tolist())\n",
        "  c_feature = df_feature.iloc[:, 1:].to_numpy()\n",
        "  features = np.concatenate((i_feature, c_feature), axis=1)\n",
        "\n",
        "  target = df['dx'].to_numpy()\n",
        "  return features, target"
      ],
      "metadata": {
        "id": "rVL9R1sKZSVm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def image_augment(df, target, count, size, rs=42):\n",
        "  df_group = df.groupby('dx')\n",
        "  group = df_group.get_group(target)\n",
        "  s=group.sample(count, axis=0, random_state=rs)\n",
        "\n",
        "  datagen = ImageDataGenerator(\n",
        "    rotation_range = 20,\n",
        "    width_shift_range = 0.1,\n",
        "    height_shift_range = 0.1,\n",
        "    horizontal_flip = True,\n",
        "    fill_mode='nearest')\n",
        "  \n",
        "  for index, row in s.iterrows():\n",
        "    image = row['image'].reshape((1, ) + row['image'].shape)\n",
        "    gen = datagen.flow(image, batch_size=size)\n",
        "    input = row.to_list()\n",
        "    for i in range(size):\n",
        "      img = next(gen)\n",
        "      input[-1] = img[0]\n",
        "      df.loc[len(df.index)] = input\n",
        "  return None"
      ],
      "metadata": {
        "id": "BOOHmibjqw8E"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prep_pipeline1(upper_size, h, w, aug_targets, aug_count, aug_size, rs=42):\n",
        "  df_o = pd.read_csv('../content/drive/MyDrive/DSE_I2100/data/HAM10000_metadata.csv')\n",
        "  image_path = r'../content/images/'\n",
        "  df_u = undersample(df_o, upper_size)\n",
        "  \n",
        "  df = img_np_convert(df_u, image_path, h, w)\n",
        "\n",
        "  df_train, df_test, df_val = my_split(df, 0.7, 0.2, 0.1, rs)\n",
        "\n",
        "  for target in aug_targets:\n",
        "    image_augment(df_train, target, aug_count, aug_size, rs)\n",
        "\n",
        "  weight, labels = weight_cal(df_train)\n",
        "\n",
        "  X_train, y_train = df_to_np1(df_train)\n",
        "  X_test, y_test = df_to_np1(df_test)\n",
        "  X_val, y_val = df_to_np1(df_val)\n",
        "  return (X_train, y_train), (X_test, y_test), (X_val, y_val), weight, labels"
      ],
      "metadata": {
        "id": "tYr4L4rkIKtd"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prep_pipeline2(upper_size, h, w, aug_targets, aug_count, aug_size, rs=42):\n",
        "  df_o = pd.read_csv('../content/drive/MyDrive/DSE_I2100/data/HAM10000_metadata.csv')\n",
        "  image_path = r'../content/images/'\n",
        "  df_u = undersample(df_o, upper_size)\n",
        "\n",
        "  df = img_np_convert_scaled(df_u, image_path, h, w)\n",
        "\n",
        "  df_train, df_test, df_val = my_split(df, 0.7, 0.2, 0.1, rs)\n",
        "\n",
        "  for target in aug_targets:\n",
        "    image_augment(df_train, target, aug_count, aug_size, rs)\n",
        "\n",
        "  weight, labels = weight_cal(df_train)\n",
        "\n",
        "  X_train, y_train = df_to_np2(df_train)\n",
        "  X_test, y_test = df_to_np2(df_test)\n",
        "  X_val, y_val = df_to_np2(df_val)\n",
        "  return (X_train, y_train), (X_test, y_test), (X_val, y_val), weight, labels"
      ],
      "metadata": {
        "id": "AH6h8XJVb_R8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h = 240\n",
        "w = 240\n",
        "aug_targets = ['mel', 'bcc']\n",
        "aug_count = 5\n",
        "aug_size = 2\n",
        "\n",
        "train_set, test_set, val_set, class_weight, labels = prep_pipeline1(3000, h, w, aug_targets, aug_count, aug_size)"
      ],
      "metadata": {
        "id": "UPqDGqCaw2gX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e35f185b-f35f-4351-ef31-3ae07de10d77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6310, 7)\n",
            "converted\n",
            "splitted\n",
            "(4416, 9)\n",
            "(4426, 9)\n",
            "(4436, 9)\n",
            "(4436, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "h = 90\n",
        "w = 90\n",
        "aug_targets = ['mel', 'bcc']\n",
        "aug_count = 5\n",
        "aug_size = 2\n",
        "\n",
        "train_set, test_set, val_set, class_weight, labels = prep_pipeline2(3000, h, w, aug_targets, aug_count, aug_size)"
      ],
      "metadata": {
        "id": "2Dq2LWARBUWI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSofw22jdpmv",
        "outputId": "ce55657c-cd30-4aae-8c7c-84b8f0b4d1c4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4436, 24320)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtlRnC09dyrH",
        "outputId": "663725a6-db65-412c-889f-ba0e010d8802"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['mel', 'bcc', 'nv', ..., 'bcc', 'bcc', 'bcc'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}